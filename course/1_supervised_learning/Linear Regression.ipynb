{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Ways of fitting a line to a set of points (samples) are here reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Setting a random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error\n",
    "\n",
    "<img src=\"./images/02_MAE.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    " \n",
    "\\begin{equation*}\n",
    "MAE = \\frac{1}{m} \\sum_{k=1}^n | y - \\hat{y} |\n",
    "\\end{equation*}\n",
    "\n",
    "### Mean Squared Error\n",
    "\n",
    "<img src=\"./images/01_MSE.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\\begin{equation*}\n",
    "MSE = \\frac{1}{2m} \\sum_{k=1}^n \\left( y - \\hat{y} \\right)^2\n",
    "\\end{equation*}\n",
    "\n",
    "NOTE: there is a factor 2 at the bottom to ease the calculus of its derivative - avoid a multiplication by 2.\n",
    "\n",
    "### Mean vs Total Squared (or Absolute) Error\n",
    "\n",
    "It doesn't really matter which one to use. Since\n",
    "\n",
    "\\begin{equation*}\n",
    "MSE = \\frac{1}{2m} \\sum_{k=1}^n \\left( y - \\hat{y} \\right)^2  \n",
    " while \n",
    "MSE = \\frac{1}{2} \\sum_{k=1}^n \\left( y - \\hat{y} \\right)^2\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "m \\cdot MSE = TSE\n",
    "\\end{equation*}\n",
    "\n",
    "the total squared error is just a multiple of the mean squared error. However, the gradient descent step consists of subtracting the gradient of the error times the learning rate Î±.\n",
    "\n",
    "Therefore, choosing between the mean squared error and the total squared error really just amounts to picking a different learning rate. NOTE: if you use an algorithm to pick the learning rate, it will return different lr for different error functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Is the direction which leads to the absolute minimum (indeed, the Error function here has only one). The derivative must be calculated against all variables.\n",
    "\n",
    "<img src=\"./images/00_GD.png\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "\n",
    "\\begin{equation*}\n",
    "a_{i+1} = a_i - \\alpha \\nabla E(a_i)\n",
    "\\end{equation*}\n",
    "\n",
    "Where:</br>\n",
    "  * $a \\in \\mathbb{R}^n$ is the sample point vector\n",
    "  * $E$ is the error function $\\mathbb{R}^n \\Rightarrow \\mathbb{R}$, and must be differentiable at $a$\n",
    "  * $\\alpha$ is the learning rate (here, we suppose it is constant - otherwise would be $\\alpha_i$ - this could happen when annealing is performed)</br>\n",
    "\n",
    "Questions:\n",
    "How should A be? Rank? Determinant? Eigenvalues? Eigenvectors?\n",
    "\n",
    "#### Limitations\n",
    "For some cases, gradient descent is relatively slow close to the minimum. For poorly conditioned convex problems, gradient descent increasingly 'zigzags' as the gradients point nearly orthogonally to the shortest direction to a minimum point.\n",
    "\n",
    "For non-differentiable functions, gradient methods are ill-defined. Therefore we must:\n",
    "use non-descent methods, (e.g. subgradient projection methods) are typically slower than gradient descent\n",
    "\"smooth\" the function, or bound the function by a smooth function. In this approach, the smooth problem is solved in the hope that the answer is close to the answer for the non-smooth problem (occasionally, this can be made rigorous)\n",
    "Mini-Batch GD\n",
    "For computational limitations, data is split into mini-batches with uniform size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# X = [[-0.60116]\n",
    "#  [-0.94159]\n",
    "#  [-0.74565]\n",
    "#  [ 0.89583]\n",
    "#  [ 2.277  ]\n",
    "#  [-1.36115]\n",
    "#  [ 1.11943]\n",
    "#  [ 0.5336 ]\n",
    "#  [ 1.79466]\n",
    "#  [ 1.79466]\n",
    "#  [-0.73591]\n",
    "#  [ 0.20174]\n",
    "#  [ 0.03256]\n",
    "#  [ 2.64837]\n",
    "#  [ 0.74678]\n",
    "#  [ 0.26851]\n",
    "#  [-2.40724]\n",
    "#  [-0.73591]\n",
    "#  [ 0.16278]\n",
    "#  [-2.29674]]\n",
    "\n",
    "# y = [ 1.54254e+00  1.94500e+00  8.11940e-01  3.04041e+00  3.63838e+00\n",
    "#   1.70969e+00  2.08990e+00  1.75136e+00  2.95265e+00  2.95265e+00\n",
    "#   1.43076e+00  1.76894e+00  1.58565e+00  3.01665e+00  2.92253e+00\n",
    "#   2.79202e+00 -1.56000e-03  1.43076e+00  1.84746e+00  1.75695e+00]\n",
    "\n",
    "# TODO: Fill in code in the function below to implement a gradient descent\n",
    "# step for linear regression, following a squared error rule. See the docstring\n",
    "# for parameters and returned variables.\n",
    "def MSEStep(X, y, W, b, learn_rate = 0.005):\n",
    "    \"\"\"\n",
    "    This function implements the gradient descent step for squared error as a\n",
    "    performance metric.\n",
    "    \n",
    "    Parameters\n",
    "    X : array of predictor features\n",
    "    y : array of outcome values\n",
    "    W : predictor feature coefficients\n",
    "    b : regression function intercept\n",
    "    learn_rate : learning rate\n",
    "\n",
    "    Returns\n",
    "    W_new : predictor feature coefficients following gradient descent step\n",
    "    b_new : intercept following gradient descent step\n",
    "    \"\"\"\n",
    "    \n",
    "    # NOTE\n",
    "    # W.shape => (1,)\n",
    "    # X.shape => (20,1) - it is a batch\n",
    "    # y.shape => (20,)\n",
    "    # type(b) => int\n",
    "    \n",
    "    # np.matmul takes two arguments (X,Y) \n",
    "    # where shapes are [(m,k),(k,n)] and returns a [(m,n)]\n",
    "    # \n",
    "    # suppose you have two vectors:\n",
    "    # \n",
    "    # case A: (m,1),(1,n)\n",
    "    # all as above\n",
    "    #\n",
    "    # case B: (m,1),(1,)  \n",
    "    # Y.shape = (1,) => second ardument is one object\n",
    "    # Y is broadcasted as (1,1) by prepending a 1\n",
    "    # matmul(X,Y).shape = (m,)\n",
    "    # \n",
    "\n",
    "    # Fill in code\n",
    "    \n",
    "    #shapes summary: (20,1) x (1,) => (20,)\n",
    "    # input shapes: (20,1) x (1,) \n",
    "    # oper. shapes: (20,1) x (1,1) => (20,1)\n",
    "    # output shape: (20,1) => (20,)\n",
    "    y_pred = np.matmul(X,W) + b\n",
    "    error = y - y_pred\n",
    "    \n",
    "    # update w_1\n",
    "    # \n",
    "    # W = W -grad(E(X))\n",
    "    # where error function E(X) = y - F(X)\n",
    "    # and grad(E(X)) = E(X) x X\n",
    "    # \n",
    "    # shapes summary: (20,) x (20,) => (1,)\n",
    "    # input shapes: (20,) x (20,) \n",
    "    # oper. shapes: (1,20) x (20,1) => (1,1)  --prepend,append rule: https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html\n",
    "    # output shape: (1,1) => (1,)\n",
    "    W_new = W + np.matmul(error, X) * learn_rate\n",
    "    \n",
    "    # update w_2\n",
    "    # \n",
    "    # b = b - sum(error)\n",
    "    # \n",
    "    # shapes summary: sum(20,) => (1,) => int\n",
    "    # \n",
    "    b_new = b + sum(error) * learn_rate\n",
    "    print ( np.matmul(X,W).shape)\n",
    "    \n",
    "    return W_new,b_new#W_new, b_new\n",
    "\n",
    "\n",
    "# The parts of the script below will be run when you press the \"Test Run\"\n",
    "# button. The gradient descent step will be performed multiple times on\n",
    "# the provided dataset, and the returned list of regression coefficients\n",
    "# will be plotted.\n",
    "def miniBatchGD(X, y, batch_size = 20, learn_rate = 0.005, num_iter = 25):\n",
    "    \"\"\"\n",
    "    This function performs mini-batch gradient descent on a given dataset.\n",
    "    \n",
    "    Parameters\n",
    "    X : array of predictor features\n",
    "    y : array of outcome values\n",
    "    batch_size : how many data points will be sampled for each iteration\n",
    "    learn_rate : learning rate\n",
    "    num_iter : number of batches used\n",
    "\n",
    "    Returns\n",
    "    regression_coef : array of slopes and intercepts generated by gradient\n",
    "      descent procedure\n",
    "    \"\"\"\n",
    "    n_points = X.shape[0]\n",
    "    W = np.zeros(X.shape[1]) # coefficients\n",
    "    b = 0 # intercept\n",
    "    \n",
    "    # run iterations\n",
    "    regression_coef = [np.hstack((W,b))]\n",
    "    for _ in range(num_iter):\n",
    "        batch = np.random.choice(range(n_points), batch_size)\n",
    "        X_batch = X[batch,:]\n",
    "        y_batch = y[batch]\n",
    "        W, b = MSEStep(X_batch, y_batch, W, b, learn_rate)\n",
    "        regression_coef.append(np.hstack((W,b)))\n",
    "    \n",
    "    return regression_coef\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # perform gradient descent\n",
    "    data = np.loadtxt('data.csv', delimiter = ',')\n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "    regression_coef = miniBatchGD(X, y)\n",
    "    \n",
    "    # plot the results\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure()\n",
    "    X_min = X.min()\n",
    "    X_max = X.max()\n",
    "    counter = len(regression_coef)\n",
    "    for W, b in regression_coef:\n",
    "        counter -= 1\n",
    "        color = [1 - 0.92 ** counter for _ in range(3)]\n",
    "        plt.plot([X_min, X_max],[X_min * W + b, X_max * W + b], color = color)\n",
    "    plt.scatter(X, y, zorder = 3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
