{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine (or ensemble) the models you have already seen in a way that makes the combination (strong learner) of these models better at predicting than the individual models (weak models).\n",
    "\n",
    "Weak learners are usually dicision trees. These models alone are high-variance and low-bias models (see below). This means that they tend to overfit the data. To combat this tendency, we introduct randomness, and indded we create **random forest** algorithms. The techniques are:\n",
    " * **Bootstrap the data** - i.e. sampling the data with replacements and fitting the model with that\n",
    " * **Subset the features** - i.e. letting the weak models to fit a subset of the available features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization Error: Bias vs Variance\n",
    "\n",
    "In supervised learning applications, the generalization error is a measure to describe how accurately a model can predict outcomes from previously unseen data. \n",
    "\n",
    "NOTE: ML algorithms work on finite sample. This imply that such error is sensible to different (or wrong) sampling.\n",
    "\n",
    "Models' expected generalization error is the expected value of such error (averaged error on multiple samples). It is made of three components:\n",
    "\n",
    "* The **bias** is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "* The **variance** is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).\n",
    "* The **irriducible error** is an error embedded with training set data, i.e. its noise. This error cannot be exacerbated.\n",
    "\n",
    "The [variance-bias dilemma](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) or problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random forest algorithm example by picking features randomly:\n",
    "\n",
    "<img src=\".\\images\\6.01_decision-tree.PNG\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging consists in pick $n$ random subsets of the data, and train $n$ models on them. When a prediction is required, then the models vote. Visually, it is equal to overlap decision trees domains and look at their overlaps.\n",
    "\n",
    "<img src=\".\\images\\6.02_ensamblebagging.PNG\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost\n",
    "\n",
    "Adaboost is similar to bagging, but each model get progressively hardly penalised on the previous models' errors (see bigger points, got wrong by previous models.\n",
    "\n",
    "<img src=\".\\images\\6.03_ensambleboosting.PNG\" style=\"width: 500px;\"/>\n",
    "\n",
    "At each step, the misclassified points are weighted to make the model 50% wrong (random classifier). Above, it is shown the model without weights, and below with weights applied.\n",
    "\n",
    "<img src=\".\\images\\6.04_ensambleboosting_02.PNG\" style=\"width: 800px;\"/>\n",
    "\n",
    "Weights are given by the accuracy of each model. Weights are given as $w=\\ln{\\frac{x}{1-x}}$ where $x$ is the accuracy\n",
    "\n",
    "<img src=\".\\images\\6.06_ensambleboosting_04.PNG\" style=\"width: 500px;\"/>\n",
    "\n",
    "In case of a model being completely accurate, the undefined weights are replaced with $\\pm \\inf$ depending if the model gets everything right or wrong.\n",
    "\n",
    "<img src=\".\\images\\6.07_ensambleboosting_05.PNG\" style=\"width: 500px;\"/>\n",
    "\n",
    "Finally, weights are summed and their signs and areas determine the classification outcome.\n",
    "\n",
    "<img src=\".\\images\\6.08_ensambleboosting_06.PNG\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
