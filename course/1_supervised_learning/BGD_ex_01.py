import numpy as np
# Setting a random seed, feel free to change it and see different solutions.
np.random.seed(42)
import math

# TODO: Fill in code in the function below to implement a gradient descent
# step for linear regression, following a squared error rule. See the docstring
# for parameters and returned variables.
def MSEStep(X, y, W, b, learn_rate = 0.005):
    """
    This function implements the gradient descent step for squared error as a
    performance metric.
    
    Parameters
    X : array of predictor features
    y : array of outcome values
    W : predictor feature coefficients
    b : regression function intercept
    learn_rate : learning rate

    Returns
    W_new : predictor feature coefficients following gradient descent step
    b_new : intercept following gradient descent step
    """
    
    # NOTE
    # W.shape => (1,)
    # X.shape => (20,1) - it is a batch
    # y.shape => (20,)
    # type(b) => int
    
    # np.matmul takes two arguments (X,Y) 
    # where shapes are [(m,k),(k,n)] and returns a [(m,n)]
    # 
    # suppose you have two vectors:
    # 
    # case A: (m,1),(1,n)
    # all as above
    #
    # case B: (m,1),(1,)  
    # Y.shape = (1,) => second ardument is one object
    # Y is broadcasted as (1,1) by prepending a 1
    # matmul(X,Y).shape = (m,)
    # 

    # Fill in code
    
    #shapes summary: (20,1) x (1,) => (20,)
    # input shapes: (20,1) x (1,) 
    # oper. shapes: (20,1) x (1,1) => (20,1)
    # output shape: (20,1) => (20,)
    y_pred = np.matmul(X,W) + b
    error = y - y_pred
    
    # update w_1
    # 
    # W = W -grad(E(X))
    # where error function E(X) = y - F(X)
    # and grad(E(X)) = E(X) x X
    # 
    # shapes summary: (20,) x (20,) => (1,)
    # input shapes: (20,) x (20,) 
    # oper. shapes: (1,20) x (20,1) => (1,1)  --prepend,append rule: https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html
    # output shape: (1,1) => (1,)
    W_new = W + np.matmul(error, X) * learn_rate
    
    # update w_2
    # 
    # b = b - sum(error)
    # 
    # shapes summary: sum(20,) => (1,) => int
    # 
    b_new = b + sum(error) * learn_rate
    print ( np.matmul(X,W).shape)
    
    return W_new,b_new#W_new, b_new


# The parts of the script below will be run when you press the "Test Run"
# button. The gradient descent step will be performed multiple times on
# the provided dataset, and the returned list of regression coefficients
# will be plotted.
def miniBatchGD(X, y, batch_size = 20, learn_rate = 0.005, num_iter = 25):
    """
    This function performs mini-batch gradient descent on a given dataset.
    
    Parameters
    X : array of predictor features
    y : array of outcome values
    batch_size : how many data points will be sampled for each iteration
    learn_rate : learning rate
    num_iter : number of batches used

    Returns
    regression_coef : array of slopes and intercepts generated by gradient
      descent procedure
    """
    n_points = X.shape[0]
    W = np.zeros(X.shape[1]) # coefficients
    b = 0 # intercept
    
    # run iterations
    regression_coef = [np.hstack((W,b))]
    for _ in range(num_iter):
        batch = np.random.choice(range(n_points), batch_size)
        X_batch = X[batch,:]
        y_batch = y[batch]
        W, b = MSEStep(X_batch, y_batch, W, b, learn_rate)
        regression_coef.append(np.hstack((W,b)))
    
    return regression_coef


if __name__ == "__main__":
    # perform gradient descent
    data = np.loadtxt('data.csv', delimiter = ',')
    X = data[:,:-1]
    y = data[:,-1]
    regression_coef = miniBatchGD(X, y)
    
    # plot the results
    import matplotlib.pyplot as plt
    
    plt.figure()
    X_min = X.min()
    X_max = X.max()
    counter = len(regression_coef)
    for W, b in regression_coef:
        counter -= 1
        color = [1 - 0.92 ** counter for _ in range(3)]
        plt.plot([X_min, X_max],[X_min * W + b, X_max * W + b], color = color)
    plt.scatter(X, y, zorder = 3)
    plt.show()