{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "Support Vector Machine is a binary classifier. There are three main ways to implement SVMs:\n",
    " * Maximum Margin Classifier (maximize the distance of the closest point (support vectors) to the separation line)\n",
    " * Classification with Inseparable Classes (maximize the distance of the support vectors and introcude the margin error)\n",
    " * Kernels (use higher dimensionality to separate points and re-project them in to the initial dimension space)\n",
    "\n",
    "## Maximum Margin Classifier\n",
    "\n",
    "Let's minimise the error of misclassified points by a perceptron.\n",
    "\n",
    "<img src=\".\\images\\5.01_SVM-error_principle.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "We penalise more the points which are further from the line, on the wrong side of it.\n",
    "\n",
    "<img src=\".\\images\\5.02_SVM-error_principle_02.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "The error is the absolute value of the distance of the points from the line (which are on the wrong side):\n",
    "\n",
    "\\begin{equation*}\n",
    "E = \\sum{d(x_i)}\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    " * $d(x)$ is the function which returns the distance of the point from the line\n",
    " * x_i are the misclassified points\n",
    " \n",
    "## Inseparable Classes\n",
    "\n",
    "\\begin{equation*}\n",
    "E = E_m + E_c\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    " * $E_c$ is the classificaiton error, as shown in the perceptron above\n",
    " * $E_m$ is the margin error\n",
    "\n",
    "### Classification Error\n",
    "\n",
    "In SVM, the error stays as above, but an internal margin is introduced as a reference line. The error is calculated by overlapping the areas on the margin at $(-1,+1)$ from the line:\n",
    "\n",
    "<img src=\".\\images\\5.03_SVM-error_principle_03.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "### Margin Error\n",
    "\n",
    "The margin error has the scope to penalize small margins and being forgetful on large margins. Therefore, margins are and margins-related error are calculated as:\n",
    "\n",
    "\\begin{equation*}\n",
    "E = |W|^2 \\\\\n",
    "M = \\frac{2}{|W|}\n",
    "\\end{equation*}\n",
    "\n",
    "<img src=\".\\images\\5.04_SVM-error_margin_01.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "#### Example 1\n",
    "\n",
    "<img src=\".\\images\\5.05_SVM-error_margin_02.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "The line equation is\n",
    "\n",
    "\\begin{equation*}\n",
    "Wx+b=0\\\\\n",
    "w_1 x_1 + w_2 x_2 + b = 0\\\\\n",
    "3 x_1 + 4 x_2 + 1 = 0\n",
    "\\end{equation*}\n",
    "\n",
    "We have the margin lines to be:\n",
    "\n",
    "\\begin{equation*}\n",
    "3 x_1 + 4 x_2 + 1 = \\pm 1\n",
    "\\end{equation*}\n",
    "\n",
    "So the margin and the error are:\n",
    "\n",
    "\\begin{equation*}\n",
    "E = |W|^2 = 25 \\\\\n",
    "M = \\frac{2}{|W|} = \\frac{2}{5}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "<img src=\".\\images\\5.06_SVM-error_margin_03.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "We pick a line equation to describe the same line, as we multiply the previous function by two:\n",
    "\n",
    "\\begin{equation*}\n",
    "Wx+b=0\\\\\n",
    "w_1 x_1 + w_2 x_2 + b = 0\\\\\n",
    "6 x_1 + 8 x_2 + 2 = 0\n",
    "\\end{equation*}\n",
    "\n",
    "We have the margin lines to be:\n",
    "\n",
    "\\begin{equation*}\n",
    "6 x_1 + 8 x_2 + 2 = \\pm 1\n",
    "\\end{equation*}\n",
    "\n",
    "So the margin and the error are:\n",
    "\n",
    "\\begin{equation*}\n",
    "E = |W|^2 = 100 \\\\\n",
    "M = \\frac{2}{|W|} = \\frac{2}{10}\n",
    "\\end{equation*}\n",
    "\n",
    "Such error value is the same as given by the $L2$ regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C parameter\n",
    "\n",
    "The C parameter tunes if give more importance to the classification (mostly correct separation of the points) or to the margin (line that has bigger margin around it). This depends on the problem and the data:\n",
    " * high $C$ to have less mistake in classification (e.g. medical problem)\n",
    " * low $C$ to have more separation from the clusters (e.g. luxury band for products)\n",
    "\n",
    "$C$ turns out being an hyper-parameter of the SVM. Its value can be fine-tuned with specific methodologies such as grid search. On the other hand, the error function E can be here optimized by a gredient descent.\n",
    "\\begin{equation*}\n",
    "E = C \\cdot E_c + E_m\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel\n",
    "\n",
    "Let's assume we have some points on a line which we can hardly classify by cutting the (linear) space in two:\n",
    "\n",
    "<img src=\".\\images\\5.07_SVM-kernel_01.PNG\" style=\"width: 700px;\" />\n",
    "\n",
    "We introduce a second dimension (planar space) and apply them a y value according to a parabola $p(x)$ $y = x^2$, and then we can use the SVM to separate the points through a line $f(x)$ (in this case $y=4$). Therefore, the two bounds are going to be $p(x) \\cap f(x)$, which happens at $x=\\pm 2$\n",
    "\n",
    "<img src=\".\\images\\5.08_SVM-kernel_02.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "#### Go higher dimensions!\n",
    "\n",
    "In higher dimensions this becomes tricky. Let's assume the points in the image below, so to plit them there are two options:\n",
    " * use a circle (equation $x^2 + y^2 = r^2$)\n",
    " * introduce a third dimension and split them by a plane (equation $x^2 + y^2 = c$)\n",
    "\n",
    "<img src=\".\\images\\5.09_SVM-kernel_03.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "It will turn out that these two methods are the same thing. We calculate possible ways of representing them on a line, to make them splitable. Among the three options shown in the image below, we can easily split the points by a function $x^2 + y^2$\n",
    "\n",
    "<img src=\".\\images\\5.10_SVM-kernel_04.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "Therefore, the equation $x^2 + y^2 = z$ is the paraboloid which passes by the 8 points, and the half-way plane intersects it and creates the circle $x^2 + y^2 = 10$)\n",
    "\n",
    "<img src=\".\\images\\5.11_SVM-kernel_05.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "The general approach is to use high-degree polynomial kernel to use high-degree polynomial function to split our data. Here, an example to degree 2, therefore a $\\mathbb{R}^2 \\Rightarrow \\mathbb{R}^5$, which is $(x,y) \\Rightarrow (x,y,x^2,xy,y^2)$\n",
    "\n",
    "<img src=\".\\images\\5.14_SVM-kernel_08.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "<img src=\".\\images\\5.12_SVM-kernel_06.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "#### Hyper parameter for Polynomial Kernel: degree\n",
    "\n",
    "An higher degree (in the example below: 3) can be pick to fit higher dimensional polylines.\n",
    "\n",
    "<img src=\".\\images\\5.13_SVM-kernel_07.PNG\" style=\"width: 400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel\n",
    "\n",
    "Radius Based Function Kernel separates points through the sum of radius functions. The sum looks like this, and the SVM separates the points accordingly.\n",
    "\n",
    "<img src=\".\\images\\5.15_SVM-RBFkernel_01.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "More precisely, if we take $n$ points (3 in this case), here is is how they look like. remember tho: These three functions can be tuned by $n$ coefficients as you see after.\n",
    "\n",
    "<img src=\".\\images\\5.16_SVM-RBFkernel_02.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "The $n$ points, if fed to the $n$ RBF functions, returns a $n$ vectors $v \\in \\mathbb{R}^N$ of distances which can be mapped in a $n$-D space. Such $n$ vectors (or points),  representing the RBF values of each point against each RBF function can be separated by a $n$ dimensional hyper-plane.\n",
    "\n",
    "<img src=\".\\images\\5.17_SVM-RBFkernel_03.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "The equation of the hyper-plane maps the coefficient of each RBF function. The costant term represents the offset of the line used by the SVM to cut the $\\sum{RBF_i}$\n",
    "\n",
    "<img src=\".\\images\\5.18_SVM-RBFkernel_04.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "<img src=\".\\images\\5.19_SVM-RBFkernel_05.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "#### Higher dimensions\n",
    "\n",
    "In higher dimensions we use as RBF a gaussian centered into a point.\n",
    "\n",
    "<img src=\".\\images\\5.20_SVM-RBFkernel_06.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "#### Hyper parameter for RBF: gamma\n",
    "\n",
    "RBF kernel can be tuned for the width of the RBF functions used. As an RBF is mostly a gaussian bell, we define the hyper-parameter $\\gamma$ in relation to the gaussian standard deviation $\\sigma$. So, $\\gamma = \\frac{1}{2\\sigma^2}$. Here we can see a large (left) vs a small (right) $\\gamma$.\n",
    "\n",
    "<img src=\".\\images\\5.21_SVM-RBFkernel_07.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "As a result, a large $\\gamma$ tends to overfit and a small $\\gamma$ to underfit:\n",
    "\n",
    "<img src=\".\\images\\5.22_SVM-RBFkernel_08.PNG\" style=\"width: 400px;\" />\n",
    "\n",
    "This works also in $n$ dimensions, but it is mathematically more complex [(link)](https://en.wikipedia.org/wiki/Gaussian_function)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
