{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to deep learning\n",
    "\n",
    "We can recall the perceptrion algorithm lessons in `..\\1_supervised_learning\\02 Classification.ipynb`. We will move from there forward. Here we remind:\n",
    "\"\"\"\n",
    "More generally, the steps can be represented by a perceptrion diagram:\n",
    " * inputs\n",
    " * weights (linear function coefficients, + bias)\n",
    " * linear function\n",
    " * step function\n",
    " * output\n",
    "\n",
    "<img src=\"../1_supervised_learning/images/2.03_perceptron_diagram.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "The way to update the function is as follows:\n",
    " * initialise function coefficients\n",
    " * evaluate if points are in the correct semi-space\n",
    " * consider only the wrongly predicted, and loops through them:\n",
    "     * if the point is positive, but predicted negative, the weights are increased\n",
    "     * if the poins is negative, but predicted positive, the weights are dicreased\n",
    "\n",
    "Formula to update weights:\n",
    "\\begin{equation*}\n",
    "W = W \\pm \\alpha X\n",
    "b = b \\pm \\alpha\n",
    "\\end{equation*}\n",
    "where $\\pm$ is applied accordingly to how the point is misclassified\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Functions\n",
    "\n",
    "Generally, error functions must be:\n",
    " * Differentiable\n",
    " * Continuous\n",
    "\n",
    "This is to be able to use gradient descent on the error functions to train our models, minimising the error.\n",
    "\n",
    "<img src=\".\\images\\0.01_disc-cont.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "This can be done by moving from discrete predictions to continuous predictions (from {yes,no} to {67.4% yes}). To do this, we use a continuous activation function to be applied to our score (e.g. **sigmoid function**). \n",
    "\n",
    "The overall process is:\n",
    "\n",
    "#### 1 calculate the score for a point\n",
    "\n",
    "\\begin{equation*}\n",
    "\\newcommand{\\vect}[1]{\\boldsymbol{#1}}\n",
    "score = \\vect{Wx}+b\\\\\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    " * $\\vect{W}$ is the weight matrix\n",
    " * $\\vect{x}$ is the feature vector\n",
    " * $\\vect{b}$ is the bias\n",
    "\n",
    "#### 2 calculate the probability of a point of being classified by applying the activation function\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where:\n",
    " * $x$ is the score calculated above (distance from the separation function)\n",
    " \n",
    "\n",
    "<img src=\".\\images\\0.03_sigmoid.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "#### Scores and Predictions\n",
    "\n",
    "Therefore, our preceptrion will calculate scores and will spit out predictions are follow:\n",
    "\n",
    "<img src=\".\\images\\0.04_sigmoid-pred.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "<img src=\".\\images\\0.05_cont-pred.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "### Multi-class activation function: SofMax\n",
    "\n",
    "Softmax is the general case of the sigmoid for more than one class.\n",
    "\n",
    "\\begin{equation*}\n",
    "P(class_a) = \\frac{e^{Z_a}}{\\sum{e^{Z_i}}}\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    " * $Z_a$ is the score of the point belonging to the $a$ class\n",
    " * $Z_i$ is the score of the point belonging to the $i$ class\n",
    " \n",
    "<img src=\".\\images\\0.06_softmax.PNG\" style=\"width: 600px;\"/>\n",
    "\n",
    "In Python, it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.35348071e-04, 9.99658510e-01, 6.14211417e-06])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    total = sum(map(np.exp,L))\n",
    "    return np.array(list(map(lambda x:np.exp(x)/total,L)))\n",
    "\n",
    "# example\n",
    "softmax([4,12,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we output each probability of a data point being of a certain class. \n",
    "\n",
    "<img src=\".\\images\\0.07_softmax-out.PNG\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Likelyhood and Cross Entropy\n",
    "\n",
    "If we have two models and we want to understand which one is best. We calculate the scores for four points, and then their probability by applying the **Softmax** function. We pick the probability of them belonging to their actual classes. We multiply them, and that is a measure to compare models. The model on the right returns higher probability for the training data.\n",
    "\n",
    "<img src=\".\\images\\0.08_max-likelyhood.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "In order to maximise the probability (and minimize the error) we want to see how Max likelyhood and error are related. First, we want to calc the probability and maximise it. However, the product many (thousands) points might be tiny (very bad!!). So we can calculate the Max likelyhood differently, as the sum of the logarithms:\n",
    "\n",
    "\\begin{equation*}\n",
    "-\\sum{ln(P_i)}\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    " * $P_i$ is the likelyhood of a single event to happen in relation to the given label.\n",
    "\n",
    "Since $0.0 \\le P_i\\le 1.0$, their $ln(P_i) \\le 0$, thus there is a minus in front of the sum. Therefore, we move from maximizing the probability to minimizing the cross entropy. We can see also the cross entropy as a way to calculate each point's error:\n",
    "\n",
    "<img src=\".\\images\\0.09_cross-entropy.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "Now, we took $P_i$ as the likelyhood of an event to happen for a given label. So we can introduce the label as a variable by expanding the forumla as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mbox{Cross-Entropy} = -\\sum{y_i ln(P_i) + (1-y_i) ln(1-P_i)}\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    " * $P_i$ is the probability of the i-th event to happen\n",
    " * $y_i$ is the label of the i-th event\n",
    "\n",
    "<img src=\".\\images\\0.10_cross-entropy.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "<img src=\".\\images\\0.11_cross-entropy.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "Here we run an example for the most likely combination (line 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = [1,1,0]\n",
    "p = [0.8,0.7,0.1]\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    r = map(lambda y,p: y*np.log(p)+(1-y)*np.log(1.0-p),Y,P)\n",
    "    return -sum(r)\n",
    "\n",
    "round(cross_entropy(y,p),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Cross-Entropy\n",
    "\n",
    "Now we assume to have $n=3$ random variables (the doors, ???representing the label of each data-point???), each with $m=3$  $categories=\\{duck,beaver,walrus\\}$, with different probabilities to happen:\n",
    "\n",
    "<img src=\".\\images\\0.12_multi-cross-entropy.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "So we have the probability matrix:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "P_{1,1} &   P_{1,2} & \\dots     &   P_{1,m} \\\\\n",
    "P_{2,1} &   P_{2,2} & \\dots     &   P_{2,m} \\\\\n",
    "\\vdots  &  \\vdots   &   \\vdots  &   \\vdots  \\\\   \n",
    "P_{n,1} &   P_{n,2} & \\dots     &   P_{n,m} \\\\\n",
    "            \\end{bmatrix}\n",
    "            }_{\\mathbf{P}}\n",
    "            \\ ,\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "y_{1,1} &   y_{1,2} & \\dots     &   y_{1,m} \\\\\n",
    "y_{2,1} &   y_{2,2} & \\dots     &   y_{2,m} \\\\\n",
    "\\vdots  &  \\vdots   &   \\vdots  &   \\vdots  \\\\   \n",
    "y_{n,1} &   y_{n,2} & \\dots     &   y_{n,m} \\\\\n",
    "            \\end{bmatrix}\n",
    "            }_{\\mathbf{y}}\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    " * $\\mathbf{P}$ is the probability matrix with n categories and m random variables\n",
    " * $\\mathbf{y}$ is the label matrix with the classified categories\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mbox{Cross-Entropy} = -\\sum_{i=1}^{n}\\sum_{j=1}^{m} y_{i,j} \\ln(P_{i,j})\n",
    "\\end{equation*}\n",
    "\n",
    "----\n",
    "\n",
    "The example shown above is coded here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4769384801388235"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = [[1,0,0],\n",
    "     [0,0,0],\n",
    "     [0,1,1]]\n",
    "p = [[0.7,0.3,0.1],\n",
    "     [0.2,0.4,0.5],\n",
    "     [0.1,0.3,0.4]]\n",
    "\n",
    "# to np array\n",
    "y = np.array(y)\n",
    "p = np.array(p)\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    r = map(lambda y,p: y*np.log(p),Y,P)\n",
    "    return -sum(r)[0]\n",
    "\n",
    "cross_entropy(y.reshape(-1,1),p.reshape(-1,1)) # arrays are processed flattened for better performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convention, we take the average cross-entropy to obtain an error function.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mbox{Cross-Entropy} = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{m} y_{i,j} \\ln(P_{i,j})\n",
    "\\end{equation*}\n",
    "\n",
    "Now, we know that $P_i = \\hat{y}$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mbox{Cross-Entropy} = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{m} y_{i,j} \\ln(\\hat{y_{i,j}})\n",
    "\\end{equation*}\n",
    "\n",
    "And $\\hat{y} = \\sigma(\\vect{Wx}+b)$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mbox{Cross-Entropy} = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{m} y_{i,j} \\ln(\\sigma(w_{i,j}x_{i,j}+b_{i}))\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    " * $n$ is the number of random variables - i.e. of samples\n",
    " * $m$ is the number of categories\n",
    " * $w_{i,j}$ is the weight of the i-th feature, j-th category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### k\n",
    "\n",
    "**The entropy of a probability distribution is as follows:**\n",
    "\n",
    "$\\sum{P(i) log P(i)}$\n",
    "\n",
    "We assume that we know the probability P for each i. The term i indicates a discrete event that could mean different things depending on the scenario you are dealing.\n",
    "\n",
    "**For continuous variables, it can be written using the integral form:**\n",
    "\n",
    "$\\int{P(x) log P(x)dx}$\n",
    "\n",
    "Here, x is a continuous variable, and P(x) is the probability density function.\n",
    "\n",
    "In both discrete and continuous variable cases, we are calculating the expectation (average) of the negative log probability which is the theoretical minimum encoding size of the information from the event x.\n",
    "\n",
    "So, the above formula can be re-written in the expectation form as follows:\n",
    "$E_{x~P} (- log P(x))$\n",
    "\n",
    "where:\n",
    " * x~P means that we calculate the expectation with the probability distribution P.\n",
    "\n",
    "**In short, the entropy tells us the theoretical minimum average encoding size for events that follow a particular probability distribution.**\n",
    "\n",
    "*Cross-entropy ≥ Entropy*\n",
    "\n",
    "Commonly, the cross-entropy is expressed using H as follows:\n",
    "_H(P, Q) = E (subscript (x~P) [- log Q(x)])_\n",
    "\n",
    "H(P, Q) means that we calculate the expectation using P and the encoding size using Q. As such, H(P, Q) and H(Q, P) is not necessarily the same except when Q=P, in which case H(P, Q) = H(P, P) = H(P) and it becomes the entropy itself.\n",
    "\n",
    "This point is subtle but essential. For the expectation, we should use the true probability P as that tells the distribution of events. For the encoding size, we should use Q as that is used to encode messages.\n",
    "\n",
    "Since the entropy is the theoretical minimum average size, the cross-entropy is higher than or equal to the entropy but not less than that.\n",
    "\n",
    "In other words, if our estimate is perfect, Q = P and, hence, H(P, Q)=H(P). Otherwise, H(P, Q) > H(P).\n",
    "he cross-entropy compares the model’s prediction with the label which is the true probability distribution. The cross-entropy goes down as the prediction gets more and more accurate. It becomes zero if the prediction is perfect. As such, the cross-entropy can be a loss function to train a classification model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
