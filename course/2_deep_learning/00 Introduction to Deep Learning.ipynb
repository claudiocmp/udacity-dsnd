{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to deep learning\n",
    "\n",
    "We can recall the perceptrion algorithm lessons in `..\\1_supervised_learning\\02 Classification.ipynb`. We will move from there forward. Here we remind:\n",
    "\"\"\"\n",
    "More generally, the steps can be represented by a perceptrion diagram:\n",
    " * inputs\n",
    " * weights (linear function coefficients, + bias)\n",
    " * linear function\n",
    " * step function\n",
    " * output\n",
    "\n",
    "<img src=\"../1_supervised_learning/images/2.03_perceptron_diagram.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "The way to update the function is as follows:\n",
    " * initialise function coefficients\n",
    " * evaluate if points are in the correct semi-space\n",
    " * consider only the wrongly predicted, and loops through them:\n",
    "     * if the point is positive, but predicted negative, the weights are increased\n",
    "     * if the poins is negative, but predicted positive, the weights are dicreased\n",
    "\n",
    "Formula to update weights:\n",
    "\\begin{equation*}\n",
    "W = W \\pm \\alpha X\n",
    "b = b \\pm \\alpha\n",
    "\\end{equation*}\n",
    "where $\\pm$ is applied accordingly to how the point is misclassified\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Functions\n",
    "\n",
    "Generally, error functions must be:\n",
    " * Differentiable\n",
    " * Continuous\n",
    "\n",
    "This is to be able to use gradient descent on the error functions to train our models, minimising the error.\n",
    "\n",
    "<img src=\".\\images\\0.01_disc-cont.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "This can be done by moving from discrete predictions to continuous predictions (from {yes,no} to {67.4% yes}). To do this, we use a continuous activation function to be applied to our score (e.g. **sigmoid function**). \n",
    "\n",
    "The overall process is:\n",
    "\n",
    "#### 1 calculate the score for a point\n",
    "\n",
    "\\begin{equation*}\n",
    "\\newcommand{\\vect}[1]{\\boldsymbol{#1}}\n",
    "score = \\vect{Wx}+b\\\\\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    " * $\\vect{W}$ is the weight matrix\n",
    " * $\\vect{x}$ is the feature vector\n",
    " * $\\vect{b}$ is the bias\n",
    "\n",
    "#### 2 calculate the probability of a point of being classified by applying the activation function\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where:\n",
    " * $x$ is the score calculated above (distance from the separation function)\n",
    " \n",
    "\n",
    "<img src=\".\\images\\0.03_sigmoid.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "#### Scores and Predictions\n",
    "\n",
    "Therefore, our preceptrion will calculate scores and will spit out predictions are follow:\n",
    "\n",
    "<img src=\".\\images\\0.04_sigmoid-pred.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "<img src=\".\\images\\0.05_cont-pred.PNG\" style=\"width: 600px;\" />\n",
    "\n",
    "### Multi-class activation function: SofMax\n",
    "\n",
    "Softmax is the general case of the sigmoid for more than one class.\n",
    "\n",
    "\\begin{equation*}\n",
    "P(class_a) = \\frac{e^{Z_a}}{\\sum{e^{Z_i}}}\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    " * $Z_a$ is the score of the point belonging to the $a$ class\n",
    " * $Z_i$ is the score of the point belonging to the $i$ class\n",
    " \n",
    "<img src=\".\\images\\0.06_softmax.PNG\" style=\"width: 600px;\"/>\n",
    "\n",
    "In Python, it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.35348071e-04, 9.99658510e-01, 6.14211417e-06])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    total = sum(map(np.exp,L))\n",
    "    return np.array(list(map(lambda x:np.exp(x)/total,L)))\n",
    "\n",
    "# example\n",
    "softmax([4,12,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we output each probability of a data point being of a certain class. \n",
    "\n",
    "<img src=\".\\images\\0.07_softmax-out.PNG\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Likelyhood\n",
    "\n",
    "If we have two models and we want to understand which one is best. We calculate the scores for four points, and then their probability by applying the **Softmax** function. We pick the probability of them belonging to their actual classes. We multiply them, and that is a measure to compare models. The model on the right returns higher probability for the training data.\n",
    "\n",
    "<img src=\".\\images\\0.08_max-likelyhood.PNG\" style=\"width: 600px;\" />\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
